Day-48 session-48(22/10/2024)
------------------------------

1. building the images --> dockerfile
2. running the images --> containers(docker compose)

-> Now, mysql is running with the root user 
-> we have to create a user and group 
-> all the related mysql directories we should give permissions to this group and user 
-> data of mysql saves in the location --> /var/lib/mysql 
/var/run/mysqld
/docker-entrypoint-initdb.d

Generally we keep the data out of the containers 


mysql 
-----
FROM mysql:8.0  # it is like debian OS 
ENV MYSQL_ROOT_PASSWORD=ExpenseApp@1 
RUN groupadd expense && \
	useradd -g expense expense && \
	chown -R expense:expense /var/lib/mysql  /var/run/mysqld /docker-entrypoint-initdb.d
ADD scripts/*.sql /docker-entrypoint-initdb.d
USER expense
    # MYSQL_DATABASE=transactions \
    # MYSQL_USER=expense \
    # MYSQL_PASSWORD=ExpenseApp@1
	

1. git clone <https url>
2. install docker and disk space 
3. docker build -t mahalakshmi2997/mysql:v1 .
4. cd ..
5. docker compose up -d 
6. docker ps 
make backend 10 seconds 
7. docker exec -it mysql bash 
8. check the application in chrome 
9. docker images 


Disadvantages of docker 
-------------------------
for example
------------
for suppose one PG has 50 rooms. If there is no water for 5 days in a PG 

If the same owner of that PG has another 5 PGs. either he get water from there or he shift persons to another PG 

1. There is no reliability since there is only one docker host 
2. There is no autoscaling 
3. There is no load balancing 
4. volumes are inside docker host -> poor volume management 
5. No secret management --> no security 
6. No communication between containers or in another docker host -> network management is not good 

so, due to these disadvantages, Orchestration comes into picture 

What is Orchestration?
----------------------
If suppose one person has 100 PGs --> He need a person to manage all at a time 
-> One host is not enough to manage 1000s of containers.so, for this we need a Orchestration tool
-> Kubernetes is the Orchestration tool 

what is Kubernetes?
-------------------
-> Kubernetes is by nature highly available, fault tolerent, and self healing.
-> It is a Orchestration platform 
-> Running the multiple containers 

-> control plane or master connects to multiple instances or nodes 

How to setup these?
-------------------
-> we have vs code to develop dockerfiles 
-> we are pushing the developed files from vscode to GitHub 
-> we are creating workstation(ec2-instance) and we pulling from gitHub to workstation and creating docker images in the workstation 
-> we are storing the docker images which we created are pushed to dockerhub 
-> nodes pull the docker images from dockerhub 
-> To pull the images from dockerhub we need to give some instructions i.e throgh the kubernetes 

-> kubectl --> k8 client command [like for SSH we have used ssh client and for mysql we used mysql command prompt]
-> kubectl is used to communicate with the kubernetes 
-> eksctl --> comamnd to create, update and delete cluster. Basically for managing cluster 

-> kubectl and eksctl are the commandline tools that are in the workstation 

How to authenticate to AWS?
---------------------------
It is through AWS CLI and we have configure it by using aws configure 

we use eksctl for cluster creation and we use kubectl for communication 

-> docker compose is a yaml file 
-> here, we develop manifest files in vscode and push it to github and pulling from github to workstation 


assignment 
------------
1. There is a docker host running 
2. No space left on device 
3. You need to add extra disk to the running instance
4. make sure docker directory /var/lib/docker is mounted to new disk 
5. migrate existing data to new mount  

setup
-------
1. create an ec2-instance(instance_type = t3.micro) make sure you have atleast 50GB storage 
2. And assign more storage to /var --> resize the disk space 
-> lsblk
-> sudo growpart /dev/nvme0n1 4
-> sudo lvextend -l +50%FREE /dev/RootVG/rootVol
-> sudo lvextend -l +50%FREE /dev/RootVG/varVol
-> sudo xfs_growfs /
-> sudo xfs_growfs /var

3. Install docker 

4. Install kubectl --> from AWS documentation 
-> curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.0/2024-09-12/bin/linux/amd64/kubectl
-> chmod +x ./kubectl 
-> sudo mv kubectl /usr/local/bin/kubectl [anyone can login and run this command]
-> kubectl version 

5. Install eksctl --> from AWS documentation
-> ARCH=amd64
-> PLATFORM=$(uname -s)_$ARCH
-> curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
-> curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
-> tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
-> sudo mv /tmp/eksctl /usr/local/bin
-> eksctl version 

6. aws configure 

Now we have to create a cluster 
--------------------------------
sample syntax:
----------------
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: eu-north-1

managedNodeGroups:      # managedNodegroups means AWS take cares everything 
  - name: ng-1
    instanceType: m5.large
    desiredCapacity: 10
    spot: true 
	
eks config file with spot instances - spot instances
------------------------------------------------------
AWS have huge data center. there may be unused capacity in data center 

spot instances --> 90% discount. when AWS requires capacity to ondemand clients they take back instances with 2 mins notice

-> when spot instance is true then we can reduce the practice cost

Can I run spot instances in production?
A) No becoz AWS takes everything, only we can put spot instances in development and testing
	
create a new folder in vscode -> k8-eksctl
-------------------------------------------
k8-eksctl --> eks.yaml


apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: expense-1
  region: us-east-1

managedNodeGroups:
  - name: expense
    instanceType: m5.large
    desiredCapacity: 3
    spot: true 
	

-> ls -l 
-> cat eks.yaml 
-> eksctl create cluster --config-file=eks.yaml 
-> (internally it creates a cloudformation stacks its just like a terraform infrastructure as a code)
-> kubectl get nodes --> we can see how many nodes are there in cluster 

In kubernetes, everything is a resources 

Resources 
-------------
namespace --> just like VPC, you will have a dedicated isolated project to create your workloads/resources -> where we can create our workloads or resources

syntax:
-------
apiVersion: 
kind: Namespace
metadata: 
	name:
	labels:
spec:

create a folder in vscode --> k8-resouces and a new repo with the same name 
=> 01-namespace.yaml 

apiVersion: v1 
kind: Namespace
metadata:
	name: expense 
	labels: 
	  project: expense 
	  environment: dev 
	  
-> kubectl apply -f 01-namespace.yaml 
-> kubectl get namespaces 
-> kubectl delete -f 01-namespace.yaml 
-> kubectl get namespaces 

pod:
----
-> It is the smallest deployable unit in kubernetes
-> It can contain one or many containers 

pod vs containers: -> interview question
-------------------
1. pod is the smallest deployable unit in kubernetes
2. pod can contain one or many containers 
3. containers in a pod can share same networks identity and storage 
-> nginx and almalinux shares the same ip address 
-> nginx writes logs into /var/log/nginx 
-> containers are temporary, if we delete the container then we may loss the logs 
-> we need logs for troubleshooting purpose 
-> Elastic search - which is used to store the logs 
-> when user hits the nginx, the nginx responsibility is to send the response to the specified users request
-> when we go to nginx and for suppose if we told it to store the logs in elastic search then nginx responsibility may deviate 
-> Then almalinux shares the same storage that can be accessed. And it pushes to the elastic search -> so, this is called as sidecar [which is ntng but extra functionality]
-> Then at this time multi-containers can be useful 
4. These are useful in sidecar and proxy patterns 


02-pod.yaml 
=============

apiVersion: v1 
kind: Pod 
metadata:
	name: nginx 
spec: 
   containers:
   # This is equivalent to docker run -d --name nginx nginx 
   - name: nginx 
     image: nginx 
	
-> kubectl apply -f 02-pod.yaml 
-> kubectl get pods 
-> kubectl exec -it nginx -- bash 

03-multi-container.yaml
=======================

kind: Pod 
apiVersion: v1 
metadata: 
	name: multi-container 
spec:
	containers:
	- name: nginx 
	  image: nginx 
	- name: almalinux 
	  image: almalinux:9
	  command: ["sleep", "1000"]
	  
	
-> pod-1 have nginx container is there 
-> pod-2 can have nginx container or not? -> yes, becoz it is in different pods 

pod-2 
------
can i create two conatiners? --> No, we will get conflicts
1. nginx 
2. nginx with name also nginx  

-> kubectl apply -f 03-multi-container.yaml 
-> kubectl get pods 

CrashLoopBackoff -> one of the popular error 
----------------
container is not able to start 

-> In the situation of pod updates may not change fields other than specific containers -> Then we have to delete it and recreate it  
-> kubectl exec -it multi-container -c almalinux -- bash
-> curl localhost  --> we will get nginx 




To delete the k8 cluster
--------------------------- 
eksctl delete cluster --config-file=eks.yaml
