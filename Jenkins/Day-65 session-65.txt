Day-65 session-65(22/11/2024)
------------------------------
create a folder - cicd-tools 

data.tf 
========


data "aws_ami" "ami_info" {

    most_recent = true
    owners = ["973714476881"]

    filter {
        name   = "name"
        values = ["RHEL-9-DevOps-Practice"]
    }

    filter {
        name   = "root-device-type"
        values = ["ebs"]
    }

    filter {
        name   = "virtualization-type"
        values = ["hvm"]
    }
}


jenkins-agent.sh 
=================

#!/bin/bash

#resize disk from 20GB to 50GB
growpart /dev/nvme0n1 4

lvextend -L +10G /dev/mapper/RootVG-homeVol
lvextend -L +10G /dev/mapper/RootVG-varVol
lvextend -l +100%FREE /dev/mapper/RootVG-varTmpVol

xfs_growfs /home
xfs_growfs /var/tmp
xfs_growfs /var

yum install java-17-openjdk -y
yum install -y yum-utils
yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
yum -y install terraform
dnf module disable nodejs -y
dnf module enable nodejs:20 -y
dnf install nodejs -y
yum install zip -y

# docker
yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
systemctl start docker
systemctl enable docker
usermod -aG docker ec2-user

# Helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh


jenkins.sh 

#!/bin/bash
curl -o /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
yum install fontconfig java-17-openjdk jenkins -y

#resize disk from 20GB to 50GB
growpart /dev/nvme0n1 4

lvextend -L +10G /dev/RootVG/rootVol
lvextend -L +10G /dev/mapper/RootVG-varVol
lvextend -l +100%FREE /dev/mapper/RootVG-varTmpVol

xfs_growfs /
xfs_growfs /var/tmp
xfs_growfs /var

systemctl daemon-reload
systemctl enable jenkins
systemctl start jenkins


main.tf 
=========

module "jenkins" {
  source  = "terraform-aws-modules/ec2-instance/aws"

  name = "jenkins"

  instance_type          = "t3.small"
  vpc_security_group_ids = ["sg-0fea5e49e962e81c9"] #replace your SG
  subnet_id = "subnet-0ea509ad4cba242d7" #replace your Subnet
  ami = data.aws_ami.ami_info.id
  user_data = file("jenkins.sh")
  tags = {
    Name = "jenkins"
  }

  # Define the root volume size and type
  root_block_device = [
    {
      volume_size = 50       # Size of the root volume in GB
      volume_type = "gp3"    # General Purpose SSD (you can change it if needed)
      delete_on_termination = true  # Automatically delete the volume when the instance is terminated
    }
  ]
}

module "jenkins_agent" {
  source  = "terraform-aws-modules/ec2-instance/aws"

  name = "jenkins-agent"

  instance_type          = "t3.small"
  vpc_security_group_ids = ["sg-0fea5e49e962e81c9"]
  subnet_id = "subnet-0ea509ad4cba242d7"
  ami = data.aws_ami.ami_info.id
  user_data = file("jenkins-agent.sh")
  tags = {
    Name = "jenkins-agent"
  }

  root_block_device = [
    {
      volume_size = 50       # Size of the root volume in GB
      volume_type = "gp3"    # General Purpose SSD (you can change it if needed)
      delete_on_termination = true  # Automatically delete the volume when the instance is terminated
    }
  ]
}

module "records" {
  source  = "terraform-aws-modules/route53/aws//modules/records"
  version = "~> 2.0"

  zone_name = var.zone_name

  records = [
    {
      name    = "jenkins"
      type    = "A"
      ttl     = 1
      records = [
        module.jenkins.public_ip
      ]
      allow_overwrite = true
    },
    {
      name    = "jenkins-agent"
      type    = "A"
      ttl     = 1
      records = [
        module.jenkins_agent.private_ip
      ]
      allow_overwrite = true
    }
  ]

}


provider.tf 
============

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.66.0"
    }
  }

  backend "s3" {
    bucket = "81s-remote-state"
    key    = "expense-tools"
    region = "us-east-1"
    dynamodb_table = "81s-locking"
  }
}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}


variables.tf 
==============

variable "zone_name" {
  type        = string
  default     = "daws81s.fun"
  description = "description"
}


readme.md
===========
Plugins:

* Pipeline stage view
* Pipeline Utility Steps
* AWS Credentials
* AWS Steps

-> search for http://jenkins.daws81s.fun:8080 in chrome 
-> login to the jenkins instance public ip in superputty 
-> df -hT 
-> sudo cat /var/lib/jenkins/secrets/initialAdminPassword 
-> we will get the password, paste it in the jenkins page 
-> Install the suggested plugins 
-> username - admin 
-> password - admin@123
-> confirm password - admin@123 
-> full name - admin 
-> email - admin@admin.com 
-> click on save and continue 
-> Instance configuration - jenkins url - http://jenkins.daws81s.fun:8080/
-> click on save and finish 
-> start using jenkins 

In dashboard -> manage jenkins ->plugins ->  Available plugins 
-> search for plugins utility steps, pipeline stage view, AWS credentials and pipeline AWS steps -> install them 
-> check the alerts 
	manage jenkins -> nodes -> click on new node 
	Node name - AGENT-1 
	Type - permanent agent 
	click on create 
	No.of executors - 3 
	Remote root directory - /home/ec2-user/jenkins-agent 
	labels - AGENT-1 
	launch method - launch agents via SSH 
		Host - jenkins-agent.daws81s.fun 
		Credentials - click on add 
			Username - ec2-user 
			password - DevOps321 
			ID - ssh-auth 
			description - ssh-auth
		Host key verification strategy - non verifying verification strategy 
		click on save 
		
-> create a repo - cicd-tools 

application -> CICD mandatory 
infra -> CICD is not mandatory for now 

-> New item -> EXPENSE -> and select a folder option and click on OK 
->In EXPENSE -> new item -> INFRA -> and select a folder option and click on OK 
->In INFRA-> new item -> DEV -> and select a folder option and click on OK

create a folder - expense-infra 

create a subfolder - 00-vpc 

Jenkinsfile 
================

pipeline {
    agent {
        label 'AGENT-1'
    }
    options{
        timeout(time: 30, unit: 'MINUTES')
        disableConcurrentBuilds()
        ansiColor('xterm')
    }
    // environment {
    //     DEBUG = 'true'
    // }

    parameters {
        choice(name: 'ACTION', choices: ['apply', 'destroy'], description: 'Select Action')
    }
    stages {
        stage('Init and Plan') {
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 00-vpc
                        terraform init -reconfigure
                        terraform plan -out=tfplan
                    """
                }
            }
        }

        stage('Apply or Destroy') {
            /* input {
                message "Should we continue to ${params.ACTION}"
                ok "Yes, we should."
            } */
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 00-vpc
                        if [ ${params.ACTION} == 'apply' ]
                        then
                            terraform apply -auto-approve tfplan
                        elif [ ${params.ACTION} == 'destroy' ]
                        then
                            terraform destroy -auto-approve
                        fi
                    """
                }
            }
        }
        stage ('Trigger SG') {
            when {
                expression { params.ACTION == 'apply' }
            }
            steps{
                build job: '10-sg', parameters: [string(name: 'ACTION', value: "apply")], wait: false
            }
        }
    }

    post {
        always{
            echo "This sections runs always"
            deleteDir()
        }
        success{
            echo "This section run when pipeline success"
        }
        failure{
            echo "This section run when pipeline failure"
        }
    }
}


parameters.tf 
==============

resource "aws_ssm_parameter" "vpc_id" {
  name  = "/${var.project_name}/${var.environment}/vpc_id"
  type  = "String"
  value = module.vpc.vpc_id
}

resource "aws_ssm_parameter" "public_subnet_ids" {
  name  = "/${var.project_name}/${var.environment}/public_subnet_ids"
  type  = "StringList"
  value = join(",", module.vpc.public_subnet_ids)
}

resource "aws_ssm_parameter" "private_subnet_ids" {
  name  = "/${var.project_name}/${var.environment}/private_subnet_ids"
  type  = "StringList"
  value = join(",", module.vpc.private_subnet_ids)
}

resource "aws_ssm_parameter" "database_subnet_ids" {
  name  = "/${var.project_name}/${var.environment}/database_subnet_ids"
  type  = "StringList"
  value = join(",", module.vpc.database_subnet_ids)
}

resource "aws_ssm_parameter" "database_subnet_group_name" {
  name  = "/${var.project_name}/${var.environment}/database_subnet_group_name"
  type  = "String"
  value = module.vpc.database_subnet_group_name
}


provider.tf 
=============

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.66.0"
    }
  }

  backend "s3" {
    bucket = "81s-remote-state-dev"
    key    = "expense-vpc"
    region = "us-east-1"
    dynamodb_table = "81s-locking"
  }
}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}


variables.tf 
================

variable "project_name" {
    default = "expense"
}

variable "environment" {
    default = "dev"
}

variable "vpc_cidr" {
    default = "10.0.0.0/16"
}

variable "common_tags" {
    default = {
        Project = "expense"
        Terraform = "true"
        Environment = "dev"
    }
}

variable "public_subnet_cidrs" {
    default = ["10.0.1.0/24", "10.0.2.0/24"]
}

variable "private_subnet_cidrs" {
    default = ["10.0.11.0/24", "10.0.12.0/24"]
}

variable "database_subnet_cidrs" {
    default = ["10.0.21.0/24", "10.0.22.0/24"]
}

variable "is_peering_required" {
    default = true
}


vpc.tf 
========

module "vpc" {
    #source = "../terraform-aws-vpc"
    source = "git::https://github.com/lakshmimungara2997/terraform-aws-vpc.git?ref=main"
    vpc_cidr = var.vpc_cidr
    project_name = var.project_name
    environment = var.environment
    common_tags = var.common_tags
    public_subnet_cidrs = var.public_subnet_cidrs
    private_subnet_cidrs = var.private_subnet_cidrs
    database_subnet_cidrs = var.database_subnet_cidrs
    is_peering_required = var.is_peering_required
}


-> In dashboard -> manage jenkins -> credentials -> system -> Global credentials  -> add credentials 
	Kind - select AWS credentials 
	ID - AWS-creds 
	Access key - 
	Secret key - 
	click on create 

-> In dashboard -> manage jenkins -> plugins -> available plugins -> search for AnsiColors -> Install it 

-> Now, restart in master jenkins 
-> sudo systemctl restart jenkins 

	
-> EXPENSE -> INFRA -> DEV -> new item 
	enter an item name - 00-vpc 
	select pipeline 
	click on OK 
	In pipeline 
		Definition - Pipeline script from scm 
		SCM - Git 
		Repository URL - expense-infra https url 
		Branch specific - */main 
		script path - 00-vpc/Jenkinsfile
	apply and save 

-> click on build now 


In sub-folder - 10-sg 

Jenkinsfile 
===========

pipeline {
    agent {
        label 'AGENT-1'
    }
    options{
        timeout(time: 30, unit: 'MINUTES')
        disableConcurrentBuilds()
        ansiColor('xterm')
    }
    // environment {
    //     DEBUG = 'true'
    // }

    parameters {
        choice(name: 'ACTION', choices: ['apply', 'destroy'], description: 'Select Action')
    }
    stages {
        stage('Init and Plan') {
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 10-sg
                        terraform init -reconfigure
                        terraform plan -out=tfplan
                    """
                }
            }
        }

        stage('Apply or Destroy') {
            /* input {
                message "Should we continue to ${params.ACTION}"
                ok "Yes, we should."
            } */
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 10-sg
                        if [ ${params.ACTION} == 'apply' ]
                        then
                            terraform apply -auto-approve tfplan
                        elif [ ${params.ACTION} == 'destroy' ]
                        then
                            terraform destroy -auto-approve
                        fi
                    """
                }
            }
        }
        stage ('Destroy VPC') {
            when {
                expression { params.ACTION == 'destroy' }
            }
            steps{
                build job: '00-vpc', parameters: [string(name: 'ACTION', value: "destroy")], wait: false
            }
        }
        stage('Parallel') {
            when {
                expression { params.ACTION == 'apply' }
            }
            parallel {
                stage('Bastion') {
                    steps {
                        build job: '20-bastion', parameters: [string(name: 'ACTION', value: "apply")], wait: false
                    }
                }
                stage('RDS') {
                    steps {
                        build job: '30-rds', parameters: [string(name: 'ACTION', value: "apply")], wait: false
                    }
                }
                stage('EKS') {
                    steps {
                        build job: '40-eks', parameters: [string(name: 'ACTION', value: "apply")], wait: false
                    }
                }
                
                stage('ECR') {
                    steps {
                        build job: '70-ecr', parameters: [string(name: 'ACTION', value: "apply")], wait: false
                    }
                }
            }
        }

        stage('Sequential') {
            when {
                expression { params.ACTION == 'apply' }
            }
            stages {
                stage('ACM') {
                    steps {
                        build job: '50-acm', parameters: [string(name: 'ACTION', value: "apply")]
                    }
                }
                stage('ALB') {
                    steps {
                        build job: '60-alb', parameters: [string(name: 'ACTION', value: "apply")], wait: false
                    }
                }
                stage('CDN') {
                    steps {
                        build job: '80-cdn', parameters: [string(name: 'ACTION', value: "apply")], wait: false
                    }
                }
            }
                
        }

    }

    post {
        always{
            echo "This sections runs always"
            deleteDir()
        }
        success{
            echo "This section run when pipeline success"
        }
        failure{
            echo "This section run when pipeline failure"
        }
    }
}


data.tf 
==========
data "aws_ssm_parameter" "vpc_id" {
  name = "/${var.project_name}/${var.environment}/vpc_id"
}


local.tf 
==========

locals {
    vpc_id = data.aws_ssm_parameter.vpc_id.value
}


main.tf 

module "mysql_sg" {
    source = "git::https://github.com/daws-81s/terraform-aws-security-group.git?ref=main"
    project_name = var.project_name
    environment = var.environment
    sg_name = "mysql"
    vpc_id = local.vpc_id
    common_tags = var.common_tags
    sg_tags = var.mysql_sg_tags
}

module "bastion_sg" {
    source = "git::https://github.com/daws-81s/terraform-aws-security-group.git?ref=main"
    project_name = var.project_name
    environment = var.environment
    sg_name = "bastion"
    vpc_id = local.vpc_id
    common_tags = var.common_tags
    sg_tags = var.bastion_sg_tags
}

module "node_sg" {
    source = "git::https://github.com/daws-81s/terraform-aws-security-group.git?ref=main"
    project_name = var.project_name
    environment = var.environment
    sg_name = "node"
    vpc_id = local.vpc_id
    common_tags = var.common_tags
    #sg_tags = var.node_sg_tags
}

module "eks_control_plane_sg" {
    source = "git::https://github.com/daws-81s/terraform-aws-security-group.git?ref=main"
    project_name = var.project_name
    environment = var.environment
    sg_name = "eks-control-plane"
    vpc_id = local.vpc_id
    common_tags = var.common_tags
    #sg_tags = var.node_sg_tags
}

module "ingress_alb_sg" {
    source = "git::https://github.com/daws-81s/terraform-aws-security-group.git?ref=main"
    project_name = var.project_name
    environment = var.environment
    sg_name = "ingress-alb"
    vpc_id = local.vpc_id
    common_tags = var.common_tags
    #sg_tags = var.node_sg_tags
}

resource "aws_security_group_rule" "ingress_alb_https" {
  type              = "ingress"
  from_port         = 443
  to_port           = 443
  protocol          = "tcp"
  cidr_blocks = ["0.0.0.0/0"]
  security_group_id = module.ingress_alb_sg.id
}

resource "aws_security_group_rule" "node_ingress_alb" {
  type              = "ingress"
  from_port         = 30000
  to_port           = 32767
  protocol          = "tcp"
  source_security_group_id = module.ingress_alb_sg.id
  security_group_id = module.node_sg.id
}

resource "aws_security_group_rule" "node_eks_control_plane" {
  type              = "ingress"
  from_port         = 0
  to_port           = 0
  protocol          = "-1"
  source_security_group_id = module.eks_control_plane_sg.id
  security_group_id = module.node_sg.id
}

resource "aws_security_group_rule" "eks_control_plane_node" {
  type              = "ingress"
  from_port         = 0
  to_port           = 0
  protocol          = "-1"
  source_security_group_id = module.node_sg.id
  security_group_id = module.eks_control_plane_sg.id
}

resource "aws_security_group_rule" "eks_control_plane_bastion" {
  type              = "ingress"
  from_port         = 443
  to_port           = 443
  protocol          = "tcp"
  source_security_group_id = module.bastion_sg.id
  security_group_id = module.eks_control_plane_sg.id
}

resource "aws_security_group_rule" "node_vpc" {
  type              = "ingress"
  from_port         = 0
  to_port           = 0
  protocol          = "-1"
  cidr_blocks = ["10.0.0.0/16"]
  security_group_id = module.node_sg.id
}

resource "aws_security_group_rule" "node_bastion" {
  type              = "ingress"
  from_port         = 22
  to_port           = 22
  protocol          = "tcp"
  source_security_group_id = module.bastion_sg.id
  security_group_id = module.node_sg.id
}

resource "aws_security_group_rule" "mysql_bastion" {
  type              = "ingress"
  from_port         = 22
  to_port           = 22
  protocol          = "tcp"
  source_security_group_id       = module.bastion_sg.id
  security_group_id = module.mysql_sg.id
}


resource "aws_security_group_rule" "bastion_public" {
  type              = "ingress"
  from_port         = 22
  to_port           = 22
  protocol          = "tcp"
  cidr_blocks = ["0.0.0.0/0"]
  security_group_id = module.bastion_sg.id
}

resource "aws_security_group_rule" "mysql_node" {
  type              = "ingress"
  from_port         = 3306
  to_port           = 3306
  protocol          = "tcp"
  source_security_group_id = module.node_sg.id
  security_group_id = module.mysql_sg.id
}


parameters.tf 
==============

resource "aws_ssm_parameter" "mysql_sg_id" {
  # /expense/dev/mysql_sg_id
  name  = "/${var.project_name}/${var.environment}/mysql_sg_id"
  type  = "String"
  value = module.mysql_sg.id
}

resource "aws_ssm_parameter" "eks_control_plane_sg_id" {
  # /expense/dev/mysql_sg_id
  name  = "/${var.project_name}/${var.environment}/eks_control_plane_sg_id"
  type  = "String"
  value = module.eks_control_plane_sg.id
}

resource "aws_ssm_parameter" "node_sg_id" {
  # /expense/dev/mysql_sg_id
  name  = "/${var.project_name}/${var.environment}/node_sg_id"
  type  = "String"
  value = module.node_sg.id
}

resource "aws_ssm_parameter" "ingress_alb_sg_id" {
  # /expense/dev/mysql_sg_id
  name  = "/${var.project_name}/${var.environment}/ingress_alb_sg_id"
  type  = "String"
  value = module.ingress_alb_sg.id
}


resource "aws_ssm_parameter" "bastion_sg_id" {
  # /expense/dev/mysql_sg_id
  name  = "/${var.project_name}/${var.environment}/bastion_sg_id"
  type  = "String"
  value = module.bastion_sg.id
}


provider.tf 
================

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.66.0"
    }
  }

  backend "s3" {
    bucket = "81s-remote-state-dev"
    key    = "expense-sg"
    region = "us-east-1"
    dynamodb_table = "81s-locking-dev"
  }
}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}


variables.tf 
============

variable "project_name" {
    default = "expense"
}

variable "environment" {
    default = "dev"
}

variable "common_tags" {
    default = {
        Project = "expense"
        Terraform = "true"
        Environment = "dev"
    }
}

variable "mysql_sg_tags" {
    default = {
        Component = "mysql"
    }
}

variable "backend_sg_tags" {
    default = {
        Component = "backend"
    }
}

variable "frontend_sg_tags" {
    default = {
        Component = "frontend"
    }
}

variable "bastion_sg_tags" {
    default = {
        Component = "bastion"
    }
}

variable "ansible_sg_tags" {
    default = {
        Component = "ansible"
    }
}


-> In dashboard -> manage jenkins -> plugins -> availableplugins -> rebuilder -> install 


-> EXPENSE -> INFRA -> DEV -> new item 
	enter an item name - 10-sg 
	select pipeline 
	click on OK 
	In pipeline 
		Definition - Pipeline script from scm 
		SCM - Git 
		Repository URL - expense-infra https url 
		Branch specific - */main 
		script path - 10-sg/Jenkinsfile
	apply and save 

-> click on build now 

what are upstreams and downstreams 
?
==================================
A) when there are dependencies between multiple jobs in jenkins we can trigger another pipeline when one pipeline is success 

-> upstream - 00-vpc 
-> downstream - 10-sg 

vpc, sg, bastion, rds, eks, acm, alb, cdn, ecr 

vpc
sg 
	bastion 
	rds 
	eks 
	acm 
	ecr 
	cdn 
	
	alb
	
In sub-folder 20-bastion 

bastion.sh 
===========

#!/bin/bash

USERID=$(id -u)
TIMESTAMP=$(date +%F-%H-%M-%S)
SCRIPT_NAME=$(echo $0 | cut -d "." -f1)
LOGFILE=/tmp/$SCRIPT_NAME-$TIMESTAMP.log
R="\e[31m"
G="\e[32m"
Y="\e[33m"
N="\e[0m"

ARCH=amd64
PLATFORM=$(uname -s)_$ARCH

VALIDATE(){
   if [ $1 -ne 0 ]
   then
        echo -e "$2...$R FAILURE $N"
        exit 1
    else
        echo -e "$2...$G SUCCESS $N"
    fi
}

if [ $USERID -ne 0 ]
then
    echo "Please run this script with root access."
    exit 1 # manually exit if error comes.
else
    echo "You are super user."
fi

# docker
yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
systemctl start docker
systemctl enable docker
usermod -aG docker ec2-user
VALIDATE $? "Docker installation"

# eksctl
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
mv /tmp/eksctl /usr/local/bin
eksctl version
VALIDATE $? "eksctl installation"


# kubectl
curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.0/2024-09-12/bin/linux/amd64/kubectl
chmod +x ./kubectl
mv kubectl /usr/local/bin/kubectl
VALIDATE $? "kubectl installation"

# kubens
git clone https://github.com/ahmetb/kubectx /opt/kubectx
ln -s /opt/kubectx/kubens /usr/local/bin/kubens
VALIDATE $? "kubens installation"


#Helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
VALIDATE $? "helm installation"

#k9S
curl -sS https://webinstall.dev/k9s | bash
VALIDATE $? "K9S installation"


Jenkinsfile 
============

pipeline {
    agent {
        label 'AGENT-1'
    }
    options{
        timeout(time: 30, unit: 'MINUTES')
        disableConcurrentBuilds()
        ansiColor('xterm')
    }
    // environment {
    //     DEBUG = 'true'
    // }

    parameters {
        choice(name: 'ACTION', choices: ['apply', 'destroy'], description: 'Select Action')
    }
    stages {
        stage('Init and Plan') {
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 20-bastion
                        terraform init -reconfigure
                        terraform plan -out=tfplan
                    """
                }
            }
        }

        stage('Apply or Destroy') {
            /* input {
                message "Should we continue to ${params.ACTION}"
                ok "Yes, we should."
            } */
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 20-bastion
                        if [ ${params.ACTION} == 'apply' ]
                        then
                            terraform apply -auto-approve tfplan
                        elif [ ${params.ACTION} == 'destroy' ]
                        then
                            terraform destroy -auto-approve
                        fi
                    """
                }
            }
        }
        

    }

    post {
        always{
            echo "This sections runs always"
            deleteDir()
        }
        success{
            echo "This section run when pipeline success"
        }
        failure{
            echo "This section run when pipeline failure"
        }
    }
}


data.tf 
===========

data "aws_ssm_parameter" "bastion_sg_id" {
  #/expense/dev/bastion_sg_id
  name = "/${var.project_name}/${var.environment}/bastion_sg_id"
}

data "aws_ssm_parameter" "public_subnet_ids" {
  #/expense/dev/public_subnet_ids
  name = "/${var.project_name}/${var.environment}/public_subnet_ids"
}

data "aws_ami" "joindevops" {

	most_recent      = true
	owners = ["973714476881"]
	
	filter {
		name   = "name"
		values = ["RHEL-9-DevOps-Practice"]
	}
	
	filter {
		name   = "root-device-type"
		values = ["ebs"]
	}

    filter {
        name   = "virtualization-type"
        values = ["hvm"]
    }
}


locals.tf 
==========

locals {
    resource_name = "${var.project_name}-${var.environment}-bastion"
    bastion_sg_id = data.aws_ssm_parameter.bastion_sg_id.value
    public_subnet_id = split(",", data.aws_ssm_parameter.public_subnet_ids.value)[0]
}


main.tf 
========

module "bastion" {
  source  = "terraform-aws-modules/ec2-instance/aws"
  
  ami = data.aws_ami.joindevops.id
  name = local.resource_name

  instance_type          = "t3.micro"
  vpc_security_group_ids = [local.bastion_sg_id]
  subnet_id              = local.public_subnet_id
  user_data = file("bastion.sh")
  tags = merge(
    var.common_tags,
    var.bastion_tags,
    {
        Name = local.resource_name
    }
  )
}



provider.tf 
=============

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.66.0"
    }
  }

  backend "s3" {
    bucket = "81s-remote-state-dev"
    key    = "expense-bastion"
    region = "us-east-1"
    dynamodb_table = "81s-locking-dev"
  }
}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}



variables.tf 
===============

variable "project_name" {
    default = "expense"
}

variable "environment" {
    default = "dev"
}

variable "common_tags" {
    default = {
        Project = "expense"
        Terraform = "true"
        Environment = "dev"
    }
}

variable "bastion_tags" {
    default = {
        Component = "bastion"
    }
}



In 40-eks sub-folder 

Jenkinsfile 
============


pipeline {
    agent {
        label 'AGENT-1'
    }
    options{
        timeout(time: 30, unit: 'MINUTES')
        disableConcurrentBuilds()
        ansiColor('xterm')
    }
    // environment {
    //     DEBUG = 'true'
    // }

    parameters {
        choice(name: 'ACTION', choices: ['apply', 'destroy'], description: 'Select Action')
    }
    stages {
        stage('Init and Plan') {
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 40-eks
                        terraform init -reconfigure
                        terraform plan -out=tfplan
                    """
                }
            }
        }

        stage('Apply or Destroy') {
            /* input {
                message "Should we continue to ${params.ACTION}"
                ok "Yes, we should."
            } */
            steps {
                withAWS(region: 'us-east-1', credentials: 'aws-creds') {
                    sh """
                        cd 40-eks
                        if [ ${params.ACTION} == 'apply' ]
                        then
                            terraform apply -auto-approve tfplan
                        elif [ ${params.ACTION} == 'destroy' ]
                        then
                            terraform destroy -auto-approve
                        fi
                    """
                }
            }
        }
    }

    post {
        always{
            echo "This sections runs always"
            deleteDir()
        }
        success{
            echo "This section run when pipeline success"
        }
        failure{
            echo "This section run when pipeline failure"
        }
    }
}


data.tf 
============

data "aws_ssm_parameter" "eks_control_plane_sg_id" {
  #/expense/dev/bastion_sg_id
  name = "/${var.project_name}/${var.environment}/eks_control_plane_sg_id"
}

data "aws_ssm_parameter" "node_sg_id" {
  #/expense/dev/bastion_sg_id
  name = "/${var.project_name}/${var.environment}/node_sg_id"
}

data "aws_ssm_parameter" "private_subnet_ids" {
  #/expense/dev/public_subnet_ids
  name = "/${var.project_name}/${var.environment}/private_subnet_ids"
}

data "aws_ssm_parameter" "vpc_id" {
  name = "/${var.project_name}/${var.environment}/vpc_id"
}

data "aws_ami" "joindevops" {

	most_recent      = true
	owners = ["973714476881"]
	
	filter {
		name   = "name"
		values = ["RHEL-9-DevOps-Practice"]
	}
	
	filter {
		name   = "root-device-type"
		values = ["ebs"]
	}

    filter {
        name   = "virtualization-type"
        values = ["hvm"]
    }
}


locals.tf 
==========

locals {
    private_subnet_ids = split(",", data.aws_ssm_parameter.private_subnet_ids.value)
    eks_control_plane_sg_id = data.aws_ssm_parameter.eks_control_plane_sg_id.value
    node_sg_id = data.aws_ssm_parameter.node_sg_id.value
}


main.tf 
==========

resource "aws_key_pair" "eks" {
  key_name   = "eks"
  # you can paste the public key directly like this
  #public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIL6ONJth+DzeXbU3oGATxjVmoRjPepdl7sBuPzzQT2Nc sivak@BOOK-I6CR3LQ85Q"
  //public_key = file("~/.ssh/eks.pub")
  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDAwrcfiQKv9v5spMRPVt0KiP+1LL2F10v3KfOC0P/9GchUoYQIeUVBSCMYBbcQdXp0dueCvo0/Jnu8VoPY6JNxjMp8KPcFPSD820/byrLX+nZSvBUyob+VjTVzaCn7B0bDNGszOfwMN9muq/CTMcMxQOIJ4jFecOgt7SgZbbNh7g6/2q5SJOuFiWZkqgsxvbAytVA3/FL0v5UU+Ba7Kh1Ugu1skQNDClvpZg+NzHHQNP6E4EWAtZSQUflPS83qtvJPF6cXj9bFb7h5kG+i2qWsB6+iujC+964XWImGr8ftVOpe6JWxRcg+C++bTJgz4sdWxCBbo/KtpzjLzVtvOpAI10V9Y+KGkPQuEVYty9Wk1HzLb13TS+iWbXXdrW0eadFl5nGsyrvE1Yen+Tae7J6ayRmjrfYw6Dio1rAmTW1Sms+Df4cESzpVsjEALfoqJmNxDJy6SLpZXQiNXhdeIAxkotnedo0fEHbsx15nx/0EJT9bHCfyh/l6g+l+4O2mjOE= user@AshDexter-T480"
  # ~ means windows home directory
}

module "eks" {
  source  = "terraform-aws-modules/eks/aws"


  cluster_name    = "${var.project_name}-${var.environment}"
  cluster_version = "1.31"

  cluster_endpoint_public_access  = true

  cluster_addons = {
    coredns                = {}
    eks-pod-identity-agent = {}
    kube-proxy             = {}
    vpc-cni                = {}
  }

  vpc_id                   = data.aws_ssm_parameter.vpc_id.value
  subnet_ids               = local.private_subnet_ids
  control_plane_subnet_ids = local.private_subnet_ids

  create_cluster_security_group = false
  cluster_security_group_id     = local.eks_control_plane_sg_id

  create_node_security_group = false
  node_security_group_id     = local.node_sg_id

  # the user which you used to create cluster will get admin access

  # EKS Managed Node Group(s)
  eks_managed_node_group_defaults = {
    instance_types = ["m6i.large", "m5.large", "m5n.large", "m5zn.large"]
  }

  eks_managed_node_groups = {
    # blue = {
    #   min_size      = 3
    #   max_size      = 10
    #   desired_size  = 3
    #   capacity_type = "SPOT"
    #   iam_role_additional_policies = {
    #     AmazonEBSCSIDriverPolicy          = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
    #     AmazonElasticFileSystemFullAccess = "arn:aws:iam::aws:policy/service-role/AmazonEFSCSIDriverPolicy"
    #     ElasticLoadBalancingFullAccess = "arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess"
    #   }
    #   # EKS takes AWS Linux 2 as it's OS to the nodes
    #   key_name = aws_key_pair.eks.key_name
    # }
    green = {
      min_size      = 3
      max_size      = 10
      desired_size  = 3
      capacity_type = "SPOT"
      iam_role_additional_policies = {
        AmazonEBSCSIDriverPolicy          = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
        AmazonElasticFileSystemFullAccess = "arn:aws:iam::aws:policy/AmazonElasticFileSystemFullAccess"
        ElasticLoadBalancingFullAccess = "arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess"
      }
      # EKS takes AWS Linux 2 as it's OS to the nodes
      key_name = aws_key_pair.eks.key_name
    }
  }

  # Cluster access entry
  # To add the current caller identity as an administrator
  enable_cluster_creator_admin_permissions = true

  tags = var.common_tags
}


provider.tf 
==============

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.75.0"
    }
  }

  backend "s3" {
    bucket = "81s-remote-state-dev"
    key    = "expense-eks"
    region = "us-east-1"
    dynamodb_table = "81s-locking-dev"
  }
}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}


variables.tf 
===============

variable "project_name" {
    default = "expense"
}

variable "environment" {
    default = "dev"
}

variable "common_tags" {
    default = {
        Project = "expense"
        Terraform = "true"
        Environment = "dev"
    }
}


In dashboard -> EXPENSE -> INFRA -> DEV -> new  item 
	enter the item name - 20-bastion 
	select pipeline 
	copy from - 10-sg 
	click on OK 
	script path - 20-bastion/Jenkinsfile
	apply and save 
	
	
In dashboard -> EXPENSE -> INFRA -> DEV -> new  item 
	enter the item name - 40-eks 
	select pipeline 
	copy from - 20-bastion
	click on OK 
	script path - 40-eks/Jenkinsfile
	apply and save 
	
-> so, now click on build with parameters from the start 00-vpc to 40-eks 